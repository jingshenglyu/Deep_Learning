{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from NN_function import *\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # create a new array, whose values are 0. Its shape is tha same as `x`.\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # calculate f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # calculate f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #the orignal value\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def cross_entropy_error(y, t):  # if one-hot encoding\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class of the 2Layer Neural Network \n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # init\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.0993, 0.1032\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "train acc, test acc | 0.10441666666666667, 0.1028\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeyklEQVR4nO3de3hU5bn38e+dExBADgkgEpSgyFEFjRTrodpWJWhR6qFasdZ2g61C7d5Fxdoi2l6WDa+2r5dUpZZqkS3FE2CLiljUvXdFCIqclaMSQIkIgRBCTvf7xwy8yWQCE83KBNfvc125mLWeZ9bcCcn8Zj1rrWeZuyMiIuGVkuwCREQkuRQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScoEFgZlNN7OdZraqnnYzs4fNbIOZrTCzM4OqRURE6hfkHsGTwNAjtOcDvaJfo4FHA6xFRETqEVgQuPtbwOdH6HIF8FePWAy0N7OuQdUjIiLxpSXxtbsBW2ssF0bX7YjtaGajiew10Lp167P69OnTJAWKiHxVLFu27DN37xSvLZlBYHHWxZ3vwt2nAdMA8vLyvKCgIMi6RES+cszso/raknnWUCHQvcZyDrA9SbWIiIRWMoNgHvCD6NlDQ4Bid68zLCQiIsEKbGjIzJ4BLgSyzawQuBdIB3D3x4D5wDBgA1AK3BxULSIiUr/AgsDdrz9KuwO3BfX6IiKSGF1ZLCIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEXKBBYGZDzewDM9tgZuPjtJ9oZovM7D0zW2Fmw4KsR0RE6gosCMwsFZgK5AP9gOvNrF9Mt18Bs919EHAd8Meg6hERkfiC3CMYDGxw903uXg7MAq6I6ePAcdHH7YDtAdYjIiJxBBkE3YCtNZYLo+tqmgiMNLNCYD4wNt6GzGy0mRWYWUFRUVEQtYqIhFaQQWBx1nnM8vXAk+6eAwwDZphZnZrcfZq757l7XqdOnQIoVUQkvIIMgkKge43lHOoO/fwYmA3g7m8DLYHsAGsSEZEYQQbBUqCXmeWaWQaRg8HzYvp8DHwLwMz6EgkCjf2IiDShwILA3SuBMcCrwFoiZwetNrP7zWx4tNsvgFFm9j7wDPBDd48dPhIRkQClBblxd59P5CBwzXUTajxeA5wbZA0iInJkurJYRCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhF2gQmNlQM/vAzDaY2fh6+lxrZmvMbLWZ/VeQ9YiISF1pQW3YzFKBqcDFQCGw1MzmufuaGn16AXcD57r7bjPrHFQ9IiISX5B7BIOBDe6+yd3LgVnAFTF9RgFT3X03gLvvDLAeERGJI8gg6AZsrbFcGF1X06nAqWb2v2a22MyGxtuQmY02swIzKygqKgqoXBGRcAoyCCzOOo9ZTgN6ARcC1wNPmFn7Ok9yn+buee6e16lTp0YvVEQkzBIKAjN73swuM7OGBEch0L3Gcg6wPU6fue5e4e6bgQ+IBIOIiDSRRN/YHwW+D6w3s0lm1ieB5ywFeplZrpllANcB82L6zAEuAjCzbCJDRZsSrElERBpBQkHg7gvd/QbgTGAL8JqZ/cvMbjaz9HqeUwmMAV4F1gKz3X21md1vZsOj3V4FdpnZGmARcIe77/py35KIiDSEuccO29fT0SwLGAncSGSIZyZwHnCau18YVIGx8vLyvKCgoKleTkTkK8HMlrl7Xry2hK4jMLMXgD7ADOA77r4j2vQ3M9O7sojIMSzRC8oecfd/xmuoL2FEROTYkOjB4r41T+s0sw5mdmtANYmISBNKNAhGufueQwvRK4FHBVOSiIg0pUSDIMXMDl8gFp1HKCOYkkREpCkleozgVWC2mT1G5OrgnwCvBFaViIg0mUSD4C7gFuCnRKaOWAA8EVRRIiLSdBIKAnevJnJ18aPBliMiIk0t0esIegG/A/oBLQ+td/eeAdUlIiJNJNGDxX8hsjdQSWRuoL8SubhMRESOcYkGQSt3f53IlBQfuftE4JvBlSUiIk0l0YPFZdEpqNeb2RhgG6DbSoqIfAUkukfwcyAT+BlwFpHJ524KqigREWk6R90jiF48dq273wGUADcHXpWIiDSZo+4RuHsVcFbNK4tFROSrI9FjBO8Bc83sWWD/oZXu/kIgVYmISJNJNAg6AruofaaQAwoCEZFjXKJXFuu4gIjIV1SiVxb/hcgeQC3u/qNGr0hERJpUokNDf6/xuCUwgsh9i0VE5BiX6NDQ8zWXzewZYGEgFYmISJNK9IKyWL2AExuzEBERSY5EjxHso/Yxgk+I3KNARESOcYkODbUNuhAREUmOhIaGzGyEmbWrsdzezK4MriwREWkqiR4juNfdiw8tuPse4N5gShIRkaaUaBDE65foqaciItKMJRoEBWb2kJmdbGY9zez3wLIgCxMRkaaRaBCMBcqBvwGzgQPAbUEVJSIiTSfRs4b2A+MDrkVERJIg0bOGXjOz9jWWO5jZq8GVJSIiTSXRoaHs6JlCALj7bnTPYhGRr4REg6DazA5PKWFmPYgzG6mIiBx7Ej0F9B7gf8zszejyBcDoYEoSEZGmlOjB4lfMLI/Im/9yYC6RM4dEROQYl+jB4n8DXgd+Ef2aAUxM4HlDzewDM9tgZvWedWRmV5uZR8NGRESaUKLHCG4HzgY+cveLgEFA0ZGeYGapwFQgH+gHXG9m/eL0awv8DHinAXWLiEgjSTQIyty9DMDMWrj7OqD3UZ4zGNjg7pvcvRyYBVwRp99vgMlAWYK1iIhII0o0CAqj1xHMAV4zs7kc/VaV3YCtNbcRXXeYmQ0Curt7zVth1mFmo82swMwKioqOuCMiIiINlOjB4hHRhxPNbBHQDnjlKE+zeJs63GiWAvwe+GECrz8NmAaQl5en01ZFRBpRg2cQdfc3j94LiOwBdK+xnEPtvYi2wADgDTMDOB6YZ2bD3b2goXWJiMgX80XvWZyIpUAvM8s1swzgOmDeoUZ3L3b3bHfv4e49gMWAQkBEpIkFFgTuXgmMAV4F1gKz3X21md1vZsODel0REWmYQG8u4+7zgfkx6ybU0/fCIGsREZH4ghwaEhGRY4CCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQCDQIzG2pmH5jZBjMbH6f9P8xsjZmtMLPXzeykIOsREZG6AgsCM0sFpgL5QD/gejPrF9PtPSDP3U8HngMmB1WPiIjEF+QewWBgg7tvcvdyYBZwRc0O7r7I3Uuji4uBnADrERGROIIMgm7A1hrLhdF19fkx8HK8BjMbbWYFZlZQVFTUiCWKiEiQQWBx1nncjmYjgTxgSrx2d5/m7nnuntepU6dGLFFERNIC3HYh0L3Gcg6wPbaTmX0buAf4hrsfDLAeERGJI8g9gqVALzPLNbMM4DpgXs0OZjYIeBwY7u47A6xFRETqEVgQuHslMAZ4FVgLzHb31WZ2v5kNj3abArQBnjWz5WY2r57NiYhIQIIcGsLd5wPzY9ZNqPH420G+voiIHF2gQSAi8kVUVFRQWFhIWVlZsks55rRs2ZKcnBzS09MTfo6CQESancLCQtq2bUuPHj0wi3cCosTj7uzatYvCwkJyc3MTfp7mGhKRZqesrIysrCyFQAOZGVlZWQ3ek1IQiEizpBD4Yr7Iz01BICIScgoCEZEYe/bs4Y9//OMXeu6wYcPYs2dPI1cULAWBiEiMIwVBVVXVEZ87f/582rdvH0RZgdFZQyLSrN330mrWbN/bqNvsd8Jx3Pud/vW2jx8/no0bNzJw4EAuvvhiLrvsMu677z66du3K8uXLWbNmDVdeeSVbt26lrKyM22+/ndGjRwPQo0cPCgoKKCkpIT8/n/POO49//etfdOvWjblz59KqVatar/XSSy/x29/+lvLycrKyspg5cyZdunShpKSEsWPHUlBQgJlx7733ctVVV/HKK6/wy1/+kqqqKrKzs3n99de/9M9DQSAiEmPSpEmsWrWK5cuXA/DGG2+wZMkSVq1adfi0zOnTp9OxY0cOHDjA2WefzVVXXUVWVlat7axfv55nnnmGP/3pT1x77bU8//zzjBw5slaf8847j8WLF2NmPPHEE0yePJkHH3yQ3/zmN7Rr146VK1cCsHv3boqKihg1ahRvvfUWubm5fP75543y/SoIRKRZO9In96Y0ePDgWufmP/zww7z44osAbN26lfXr19cJgtzcXAYOHAjAWWedxZYtW+pst7CwkO9973vs2LGD8vLyw6+xcOFCZs2adbhfhw4deOmll7jgggsO9+nYsWOjfG86RiAikoDWrVsffvzGG2+wcOFC3n77bd5//30GDRoU99z9Fi1aHH6cmppKZWVlnT5jx45lzJgxrFy5kscff/zwdty9zqmg8dY1BgWBiEiMtm3bsm/fvnrbi4uL6dChA5mZmaxbt47Fixd/4dcqLi6mW7fIPbueeuqpw+svueQSHnnkkcPLu3fv5pxzzuHNN99k8+bNABoaanLuVK+bT/Gnmyn5dDNV+3aCQ1HHs9h84lUAnP3+PZjXvvfOJ52+zsfdLielqpy8lRPrbHZbl4vY1vVi0iv2MWj17+q0b+16KTu6fIMWBz/jjLUP1WnfkjOcndlDyCzdzoAPH6nTvvHEq9nV8Uzalmym74Y/1Wlfn3sDu9v1p93edfTe9Nc67etO/hF7255Cx90rOOWjWXXaV5/6U/ZndqfTrqXkbn2xTvuKPj+nrGVnuhT9Lydt+0ed9uX97qQ8oz0nfPJPcj5ZWKd92YBfUZWWSfftL9N153/XaV9yxm/BUjipcC5dPnunVlt1SjoFp98HQM+PZpO9e3mt9oq0Nrw34JcA9No8gw7Fa2u1l7XoyIq+4wDos/HPHLdvY6320lZdWdV7LAD9P5xK69Jttdr3tT6Jtb1uAeC0db+nVVntu+vtOe5UPuz5QwAGrvlPMsqLa7V/3v40NvS4HoAzV95PWlXtT5xf5d+9yjNGU7x3L1WpLUmrLKVFed03vAMtO1OdkkFa5X5alO+O096F6pR00itLyCivezpnaavjcUsjvWIvGRU1D0YbbVLS+NqQIfQfMID8Sy/l8suG1Xru0KFDeeyxxzj99NPp3bs3Q4YMqbP9RE2cOJFrrrmGbt26MWTIkMibvDu/vOcebrv1Nvr360tqinHXnXdw1fU/YNq0aXz3u9+lurqazp0789prr33h1z5EQVBdDSnRHaPVc2DXeso++5iDu7ZgxYV81LIvU9v9gs2f7eeZPbfQ0faR6ekU0Q53Y9lHzqSlvQB4M+NtUmJuwvavLRk8vPhEWlDOwoy367z8oi1teaKqCx3Zy9w47S9v6cR/VXWgG0XMitM+Z3MOc6ozOcUK+Ut63fbZm3vyanUaZ9gGHonT/tfN/fjv6irOSVnN5LS67X/aNIhlXso3U97lvjjtUzcNYY3v5vKUd7krTvuDmy7kIz+ea1KX8bPUuu0PbHyfnXTgptRl/Fuc9ns3rqCETG5NLeD6OO3jN66gmhR+kbaUK1Nqt1eSzp3rVwDwq7SlXJpSUKu9mLbc+UGk/YG0JZyfsiqmPYs710Ta/5D+DmfZ+lrtn/oJ3LnqGwBMS19CX/u4VvtH/il3rjgHgKfTF3OS1b7lxvrqPdz53pkAPJ+xmM7UfrNa+dEB7l0WGR9/OeNtWlA7CL7Sv3sDbqJobymlVHMcpZxg++s8v+hgKQeopD2lHB+n/ZODpZSTTgfbTxfitR+gglSyrJROMe1pVDHhwceoxuhqu+hkezlv2gOUbV9DdUo61SkZPPm3F0lLMTKsitTUVNLS0qiu9sPHAbKzs1m16v//To0bN45qdyqqqqmsqqai2qmsci4672u8v/gNUqrLSfEK0vwHfL5jE4XVWdz5u4cZYJFP/3toQ8nBSvLz88nPz6/z/XwZ5h737pHNVl5enhcUFBy9YzyrXoDt70FxIdV7PqZq91ZKWnZl5oAn2PTZfm778EecXLmRz/w4tnk22z2LZd6Hf3a4mp7ZrRnceiedunSl6wkn0q1DJikpugReJAifb9tM7959kvTqTmX0Tbqy2rHyElIrS7HqclKrK0j1CgxnXXXkBown2U7a2X6qPIVy0qggjYqUFuxJzSY1xcisKiat+iCpXkG6V5JOJQdowRbvAkAf20o6lVRYGlWWTpWlU57WmoqM9pGgoZKU9HTSUlNJT0lJ6H1n7dq19O3bt9Y6M1vm7nnx+odmj+Dtjbtov+DPnLL3HT6xbD6u7Mh278264u78efuHHH9cS8qyf0OnTl3o3rkjPTu1pm92Gy7u0IpfpepQikhT2rvDyEhL3t9dRs2F1h2BmLNz3BkAVFY5XgZlFQegqpyUqnJaVVeQQRl7DMqrqulSXUwLP0iVpVGdmkFVSivS0zM5uVUb0lKNVHpjqelk1HsQuEU96xtPaILgw0/3MXXvaE7IHk9up7bkZrcmN7s13+3Umv/Iak3rFqH5UYjIl2VGCpCRZtCmPVD7SuJ04ORDC9W9wFJJqfeNPjWwMhMVmne/G752Ij845yTNaCgiTSul+b/NNv8KG0mahndEROLSu6OISMgpCEREYnyZaagB/vCHP1BaWtqIFQVLQSAiEiNsQRCaYwQicgz7y2V11/W/EgaPgvJSmHlN3faB34dBN8D+XTD7B7Xbbq57lXtNsdNQT5kyhSlTpjB79mwOHjzIiBEjuO+++9i/fz/XXnsthYWFVFVV8etf/5pPP/2U7du3c9FFF5Gdnc2iRYtqbfv+++/npZde4sCBA3z961/n8ccfx8zYsGEDP/nJTygqKiI1NZVnn32Wk08+mcmTJzNjxgxSUlLIz89n0qRJDf3pHZWCQEQkRuw01AsWLGD9+vUsWbIEd2f48OG89dZbFBUVccIJJ/CPf0SCpbi4mHbt2vHQQw+xaNEisrOz62x7zJgxTJgwAYAbb7yRv//973znO9/hhhtuYPz48YwYMYKysjKqq6t5+eWXmTNnDu+88w6ZmZmNNrdQLAWBiDR/R/oEn5F55PbWWUfdAziaBQsWsGDBAgYNGgRASUkJ69ev5/zzz2fcuHHcddddXH755Zx//vlH3daiRYuYPHkypaWlfP755/Tv358LL7yQbdu2MWLECABatmwJRKaivvnmm8nMzAQab9rpWAoCEZGjcHfuvvtubrnlljpty5YtY/78+dx9991ccsklhz/tx1NWVsatt95KQUEB3bt3Z+LEiZSVlVHfVD9BTTsdSweLRURixE5DfemllzJ9+nRKSkoA2LZtGzt37mT79u1kZmYycuRIxo0bx7vvvhv3+YccutdAdnY2JSUlPPfccwAcd9xx5OTkMGfOHAAOHjxIaWkpl1xyCdOnTz984FlDQyIiTSQrK4tzzz2XAQMGkJ+fz5QpU1i7di3nnBOZTbZNmzY8/fTTbNiwgTvuuIOUlBTS09N59NFHARg9ejT5+fl07dq11sHi9u3bM2rUKE477TR69OjB2WeffbhtxowZ3HLLLUyYMIH09HSeffZZhg4dyvLly8nLyyMjI4Nhw4bxwAMPNPr3G67ZR0XkmBBv9kxJXENnH9XQkIhIyCkIRERCTkEgIs3SsTZs3Vx8kZ+bgkBEmp2WLVuya9cuhUEDuTu7du06fB1ConTWkIg0Ozk5ORQWFlJUVJTsUo45LVu2JCcnp0HPURCISLOTnp5Obm5usssIjUCHhsxsqJl9YGYbzGx8nPYWZva3aPs7ZtYjyHpERKSuwILAzFKBqUA+0A+43sz6xXT7MbDb3U8Bfg/8Z1D1iIhIfEHuEQwGNrj7JncvB2YBV8T0uQJ4Kvr4OeBbppsKi4g0qSCPEXQDttZYLgS+Vl8fd680s2IgC/isZiczGw2Mji6WmNkHX7Cm7NhtNxOqq2FUV8M119pUV8N8mbpOqq8hyCCI98k+9lywRPrg7tOAaV+6ILOC+i6xTibV1TCqq+Gaa22qq2GCqivIoaFCoHuN5Rxge319zCwNaAcEM72eiIjEFWQQLAV6mVmumWUA1wHzYvrMA26KPr4a+KfrChIRkSYV2NBQdMx/DPAqkApMd/fVZnY/UODu84A/AzPMbAORPYHrgqon6ksPLwVEdTWM6mq45lqb6mqYQOo65qahFhGRxqW5hkREQk5BICIScqEJgqNNd5EMZtbdzBaZ2VozW21mtye7pprMLNXM3jOzvye7lkPMrL2ZPWdm66I/t3OSXROAmf179P9wlZk9Y2YNm/6x8eqYbmY7zWxVjXUdzew1M1sf/bdDM6lrSvT/cYWZvWhm7ZtDXTXaxpmZm1l2c6nLzMZG38dWm9nkxnq9UARBgtNdJEMl8At37wsMAW5rJnUdcjuwNtlFxPi/wCvu3gc4g2ZQn5l1A34G5Ln7ACInRwR94kN9ngSGxqwbD7zu7r2A16PLTe1J6tb1GjDA3U8HPgTubuqiiF8XZtYduBj4uKkLinqSmLrM7CIiszGc7u79gf/TWC8WiiAgsekumpy773D3d6OP9xF5U+uW3KoizCwHuAx4Itm1HGJmxwEXEDnbDHcvd/c9ya3qsDSgVfR6mEzqXjPTJNz9Lepei1NzKpengCubtCji1+XuC9y9Mrq4mMi1RkmvK+r3wJ3EucC1KdRT10+BSe5+MNpnZ2O9XliCIN50F83iDfeQ6Myrg4B3klvJYX8g8odQnexCaugJFAF/iQ5ZPWFmrZNdlLtvI/Lp7GNgB1Ds7guSW1UtXdx9B0Q+fACdk1xPPD8CXk52EQBmNhzY5u7vJ7uWGKcC50dnan7TzM5urA2HJQgSmsoiWcysDfA88HN339sM6rkc2Onuy5JdS4w04EzgUXcfBOwnOcMctUTH3K8AcoETgNZmNjK5VR07zOweIsOkM5tBLZnAPcCEZNcSRxrQgcgw8h3A7MaapDMsQZDIdBdJYWbpREJgpru/kOx6os4FhpvZFiLDaN80s6eTWxIQ+X8sdPdDe03PEQmGZPs2sNndi9y9AngB+HqSa6rpUzPrChD9t9GGFL4sM7sJuBy4oZnMKnAykUB/P/r7nwO8a2bHJ7WqiELgBY9YQmRvvVEOZIclCBKZ7qLJRdP8z8Bad38o2fUc4u53u3uOu/cg8rP6p7sn/ROuu38CbDWz3tFV3wLWJLGkQz4GhphZZvT/9Fs0g4PYNdScyuUmYG4SaznMzIYCdwHD3b002fUAuPtKd+/s7j2iv/+FwJnR371kmwN8E8DMTgUyaKQZUkMRBNEDUoemu1gLzHb31cmtCoh88r6RyCfu5dGvYckuqpkbC8w0sxXAQOCBJNdDdA/lOeBdYCWRv6ukTFFgZs8AbwO9zazQzH4MTAIuNrP1RM6EmdRM6noEaAu8Fv3df6yZ1JV09dQ1HegZPaV0FnBTY+1FaYoJEZGQC8UegYiI1E9BICIScgoCEZGQUxCIiIScgkBEJOQUBCIBM7MLm9MMriKxFAQiIiGnIBCJMrORZrYkenHT49H7MZSY2YNm9q6ZvW5mnaJ9B5rZ4hpz6XeIrj/FzBaa2fvR55wc3XybGvdRmHlojhgzm2Rma6LbabRphUUaQkEgAphZX+B7wLnuPhCoAm4AWgPvuvuZwJvAvdGn/BW4KzqX/soa62cCU939DCLzDe2Irh8E/JzI/TB6AueaWUdgBNA/up3fBvtdisSnIBCJ+BZwFrDUzJZHl3sSmdjrb9E+TwPnmVk7oL27vxld/xRwgZm1Bbq5+4sA7l5WYw6dJe5e6O7VwHKgB7AXKAOeMLPvAs1ivh0JHwWBSIQBT7n7wOhXb3efGKffkeZkOdKUwAdrPK4C0qJzYA0mMvvslcArDaxZpFEoCEQiXgeuNrPOcPg+vycR+Ru5Otrn+8D/uHsxsNvMzo+uvxF4M3oviUIzuzK6jRbR+e3jit6Hop27zycybDQwiG9M5GjSkl2ASHPg7mvM7FfAAjNLASqA24jc/Ka/mS0DiokcR4DIdM6PRd/oNwE3R9ffCDxuZvdHt3HNEV62LTDXIje6N+DfG/nbEkmIZh8VOQIzK3H3NsmuQyRIGhoSEQk57RGIiISc9ghEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTk/h9S4eYMODIi1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  \n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "        \n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 784 is out of bounds for axis 0 with size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9ebed2d455a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# calculate the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#grad = network.gradient(x_batch, t_batch) # high speed for gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf58b889edbb>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a80707569e79>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# calculate f(x+h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 784 is out of bounds for axis 0 with size 784"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# Hyperparameter\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # get the mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # calculate the gradient\n",
    "    grad = network.\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch) # high speed for gradient descent\n",
    "    \n",
    "    # update the weights and bias\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # learning process\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
